{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# K-means\n"
      ],
      "metadata": {
        "id": "dX_bUjnwfHel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from plotnine import *\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "P6_hpNyAfFd4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Together\n",
        "### Algorithm\n",
        "\n",
        "\n",
        "1. Randomly (or, in order to make convergence quicker, cleverly) choose K centroids in the feature space\n",
        "2. Assign each data point to the centroid/cluster closest to it\n",
        "3. Recalculate each centroid by taking the mean (for each predictor) of all the data points in each cluster.\n",
        "4. Repeat Steps 2 and 3 until convergence\n",
        "    - either cluster assignments don't change from step to step OR\n",
        "    - the centroid doesn't change much from step to step\n",
        "\n",
        "### Assumptions reminder\n",
        "One thing to keep in mind with K-Means is that is assumes *spherical* variance within each cluster. That means that K-means behaves as if--within each cluster--all predictors have the same variance. Roughly, this means that we could easily draw a sphere (or circle) around each of our clusters, even if it's not perfectly spherical.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1RHRfcPIjIZ_-IMOE00gyzVadlaGxXPh8\" width=350px />"
      ],
      "metadata": {
        "id": "UOlomo8SVAx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised learning has a slightly easier workflow!\n",
        "\n",
        "Previous Workflow (Supervised):\n",
        "1. Separate your data into X (predictors) and y (outcome), and maybe do some model validation set up.\n",
        "2. Create an Empty Model.\n",
        "3. call .fit() using your training data\n",
        "4. call .predict() on ANY X data (train or test) to get the model prediction for that data.\n",
        "5. Assess the model\n",
        "\n",
        "New workflow for unsupervised:\n",
        "1. Load in our data + z-score\n",
        "2. Create Empty Model\n",
        "3. Fit  Model\n",
        "4. Assess your model (using `.predict()`/`labels_` and by looking at plots, or something like a silhouette score)\n",
        "\n",
        "TODO: Discussion\n",
        "\n",
        "You may notice that there is no model validation step. Why don't we care about model validation and data leakage in unsupervised learning?\n",
        "\n",
        "You may also notice that z-scoring still must be done. Why do we still care about z-scoring in K-Means?"
      ],
      "metadata": {
        "id": "EV_VjCCZYLSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the data + standardize\n",
        "beyonce = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/Beyonce_data.csv\")\n",
        "\n",
        "predictors = [\"energy\", \"danceability\", \"valence\"]\n",
        "\n",
        "X = beyonce[predictors]\n",
        "# notice that there is no y\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 2. Create empty model\n",
        "km = #\n",
        "\n",
        "# 3. Fit model + predict\n",
        "labels = km.fit_predict(X_scaled)\n",
        "\n",
        "# 4. Assess model\n",
        "silhouette_avg = silhouette_score(X_scaled, labels)\n",
        "print(silhouette_avg)\n",
        "\n",
        "# Add cluster labels to the original DataFrame\n",
        "X[\"clusters\"] = labels\n",
        "\n",
        "# 5: Plot the results. We are plotting each combo of predictors\n",
        "print(ggplot(X, aes(x=\"energy\", y=\"danceability\", color=\"factor(clusters)\")) +\n",
        "      geom_point() + theme_minimal() +\n",
        "      labs(x=\"Energy\", y=\"Danceability\", title=\"KMeans Clustering Results for K = 4\",\n",
        "           color=\"Clusters\"))\n",
        "\n",
        "print(ggplot(X, aes(x=\"energy\", y=\"valence\", color=\"factor(clusters)\")) +\n",
        "      geom_point() + theme_minimal() +\n",
        "      labs(x=\"Energy\", y=\"Valence\", title=\"KMeans Clustering Results for K = 4\",\n",
        "           color=\"Clusters\"))\n",
        "\n",
        "print(ggplot(X, aes(x=\"valence\", y=\"danceability\", color=\"factor(clusters)\")) +\n",
        "      geom_point() + theme_minimal() +\n",
        "      labs(x=\"Valence\", y=\"Danceability\", title=\"KMeans Clustering Results for K = 4\",\n",
        "           color=\"Clusters\"))\n"
      ],
      "metadata": {
        "id": "_xhN9GqiYOt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try a new method: using the *data* to select K. This code aims to cluster wines based on `citric.acid` and `residual_sugar`."
      ],
      "metadata": {
        "id": "IDyLRdoPh9Fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/wineLARGE.csv\")\n",
        "\n",
        "# drop and reset rows\n",
        "wine.dropna(inplace = True)\n",
        "wine.reset_index(inplace = True)\n",
        "\n",
        "# grab data we want to cluster\n",
        "feats = [\"citric.acid\", \"residual.sugar\"]\n",
        "\n",
        "X = wine[feats]\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# create dictionary to\n",
        "metrics = {\"sil\": [], \"k\": []}\n",
        "\n",
        "for i in range(2,20):\n",
        "    km = #\n",
        "    labels = km.fit_predict(X_scaled)\n",
        "    sil = silhouette_score(X_scaled, labels)\n",
        "\n",
        "    metrics[\"sil\"].append(sil)\n",
        "    metrics[\"k\"].append(i)\n",
        "\n",
        "df = pd.DataFrame(metrics)\n"
      ],
      "metadata": {
        "id": "JHs1aZI3gd_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ggplot(df, aes(x = \"k\", y = \"sil\")) +\n",
        "  geom_line() + theme_minimal() +\n",
        "    labs(x = \"K\", y = \"Mean Silhouette Score\",\n",
        "         title = \"Silhouette Scores for Different Ks\"))"
      ],
      "metadata": {
        "id": "hvjoAWixhfPv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans(7)\n",
        "labels = km.fit_predict(X_scaled)\n",
        "\n",
        "X[\"cluster\"] = labels\n",
        "print(ggplot(X, aes(x = \"citric.acid\", y = \"residual.sugar\", color = \"factor(cluster)\")) +\n",
        "      geom_point() +\n",
        "      theme_minimal() +\n",
        "      scale_color_discrete(name = \"Cluster\") +\n",
        "      labs(x = \"Citric Acid\",\n",
        "           y = \"Residual Sugar\",\n",
        "           title = \"7 Cluster Solution\"))"
      ],
      "metadata": {
        "id": "AfXwvDIVhuba"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ICA"
      ],
      "metadata": {
        "id": "88a7cCkZlLRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.\n",
        "\n",
        "TODO: Using the purchases.csv dataset, select 2 features and make clusters using K-Means"
      ],
      "metadata": {
        "id": "XELJPRTzlL3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/purchases.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Hvtqnbp1lXb6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Select 2 features and make K-Means clusters\n",
        "\n"
      ],
      "metadata": {
        "id": "x-54oWF0lbAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.\n",
        "\n",
        "In this ICA, you'll be writing a k-means algorithm from scratch. Some steps are done for you.\n",
        "\n",
        "Your function, `KM()` should take in two arguments:\n",
        "\n",
        "- `df` a dataframe with all of your data.\n",
        "- `k` the number of clusters to fit.\n",
        "\n",
        "and apply K-Means to it. Remember that the steps of K means are:\n",
        "\n",
        "**1**. DONE FOR YOU. Randomly select k centroids.\n",
        "- I recommend choosing `k` random data points from `df`. You can do this by using `np.random.choice(range(0,df.shape[0]), k)` to select the indices for `k` randomly selected rows. THEN use those indices to grab the chosen rows from df and store them. Technically, you are also able to start with any random point in your space, even if it is not a member of your df, but we will use this simplified version.\n",
        "\n",
        "**2**. Assign each data point from `df` to the closest centroid.\n",
        "- You'll need to calculate the distance between each data point and each centroid. Perhaps try using `np.linalg.norm()` (see Hint 3).\n",
        "    - I recommend storing cluster/centroid membership by having a dictionary with one key for each cluster/centroid, and the value is a list of row indices pertaining to the data points in each cluster (see HINT 1 for an example of this)\n",
        "\n",
        "**3**. Re-calculate the cluster mean/centroid\n",
        "- For each centroid/cluster, find the mean value for each predictor/feature by taking the mean for that feature from all the data points assigned to the centroid/cluster. (see Hint 2)\n",
        "\n",
        "**4**. DONE FOR YOU. Repeat Steps 2-3 until the change in centroid positions are all less than 0.0001\n",
        "- in other words, calculate how far each centroid moved. If all of them moved less than 0.0001 units, then stop.\n",
        "\n",
        "**5**. DONE FOR YOU. Return the cluster assignments by returning the dictonary of the clusters and their memberships that you create in #2."
      ],
      "metadata": {
        "id": "t8zPvAZzWOXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HINT 1:\n",
        "\n",
        "You can store your cluster memberships like this (in this case k = 3, and there are only 20 datapoints, but your function should take any k, and any number of data points):\n",
        "\n",
        "```\n",
        "clust = {0: [0,7,4,5,12,18,20],\n",
        "         1: [10,8,3,2,14,17,19],\n",
        "         2: [1,6,7,9,11,13,15,16]}\n",
        "```\n",
        "      "
      ],
      "metadata": {
        "id": "-nHwtJCbXRci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HINT 2:\n",
        "\n",
        "If a cluster contained the following data points:\n",
        "\n",
        "|           | X1 | X2 | X3 |\n",
        "|-----------|----|----|----|\n",
        "| Person 1  | 5  | 2  | 9  |\n",
        "| Person 2  | 2  | 3  | 2  |\n",
        "| Person 3  | 1  | 6  | 1  |\n",
        "| Person 4  | 7  | 1  | 4  |\n",
        "| Person 5  | 3  | 2  | 5  |\n",
        "| Person 6  | 1  | 1  | 8  |\n",
        "| Person 7  | 7  | 0  | 6  |\n",
        "| Person 8  | 0  | 7  | 2  |\n",
        "| Person 9  | 2  | 3  | 7  |\n",
        "| Person 10 | 4  | 6  | 1  |\n",
        "\n",
        "\n",
        "Then the centroid for that cluster would be [a,b,c] where a, b, and c are the means of each column X1, X2, and X3:\n",
        "\n",
        "a = (5 + 2 + 1 + 7 + 3 + 1 + 7 + 0 + 2 + 4)/10\n",
        "\n",
        "b = (2 + 3 + 6 + 1 + 2 + 1 + 0 + 7 + 3 + 6)/10\n",
        "\n",
        "c = (9 + 2 + 1 + 4 + 5 + 8 + 6 + 2 + 7 + 1)/10\n"
      ],
      "metadata": {
        "id": "6rH3B1oVXRz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HINT 3:\n",
        "\n",
        "To calculate the distance between two vectors, you can use:\n",
        "\n",
        "```\n",
        "distance_ab = np.linalg.norm(a-b)\n",
        "\n",
        "```\n",
        "\n",
        "where `a` and `b` are the two vectors."
      ],
      "metadata": {
        "id": "xB39JmU8XTw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HINT 4:\n",
        "\n",
        "The `np.argmin()` function takes in a list (or array) of values, and returns the *index* of the smallest one.\n",
        "\n",
        "For example:\n",
        "\n",
        "```\n",
        "my_list = [1,6,2,5,0]\n",
        "\n",
        "np.argmin(my_list)\n",
        "```\n",
        "\n",
        "this code would return 4, because the smallest value (0) in `my_list` is at index 4."
      ],
      "metadata": {
        "id": "mNhG67c5XWLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_centroids(df,k):\n",
        "    # DONE, DON'T CHANGE ANYTHING\n",
        "\n",
        "    '''\n",
        "    use the row nums in c to grab the k data points and store them in centroids.\n",
        "    centroids should be a list of rows (each row contains the data point you chose)\n",
        "    to be a cluster center\n",
        "    '''\n",
        "\n",
        "    c = np.random.choice(range(0,df.shape[0]), k)\n",
        "    centroids = [df.iloc[l] for l in c]\n",
        "    return(centroids)\n",
        "\n",
        "def choose_closest_cluster(centroids, df, k):\n",
        "    '''\n",
        "    Create a dictionary, clust, that stores the row numbers of all the data points\n",
        "    in each cluster.\n",
        "\n",
        "    e.g. {0: [0,7], 1: [1,6,12,13], 2: [4,5], 3: [3,8,9], 4: [2,10,11]}\n",
        "\n",
        "    Loop through each data point, and calculate the distance between that data point\n",
        "    and all the cluster centers (centroids), and store them in a list. Remember\n",
        "    np.linalg.norm()!\n",
        "\n",
        "    Then, use np.argmin() to figure out which centroid is the closest, and assign\n",
        "    the data point to that cluster by adding it's row number (stored in dataPoint)\n",
        "    to the clust dictionary.\n",
        "    '''\n",
        "\n",
        "    # creating empty clust dictionary\n",
        "    clust = {}\n",
        "    for c in range(0,k):\n",
        "        clust[c] = []\n",
        "\n",
        "    # loop through each row in df\n",
        "    for dataPoint in range(0,df.shape[0]):\n",
        "        pass\n",
        "        # TODO: calculate the distance between current data point and each centroid\n",
        "\n",
        "\n",
        "        # TODO: find the centroid that's closest\n",
        "\n",
        "\n",
        "        # TODO: add dataPoint to that cluster\n",
        "\n",
        "\n",
        "    return(clust)\n",
        "\n",
        "def recalculate_cluster_mean(clust, df, k):\n",
        "    '''\n",
        "    This function takes in a dictionary of cluster memberships and the data\n",
        "    and returns the NEW cluster centers, stored in new_centroids. Cluster centers\n",
        "    are calculated by taking the mean of each feature for all the data points in\n",
        "    each cluster.\n",
        "\n",
        "    new_centroids should be a list of arrays that represent the new cluster centers.\n",
        "\n",
        "    Hint: what happens when you call .mean() on a dataframe?\n",
        "    '''\n",
        "\n",
        "    new_centroids = [[] for c in range(0,k)] #create an empty list of k 0's\n",
        "\n",
        "    for c in range(0,k):\n",
        "        # TODO: calculate the center/mean of cluster c\n",
        "        pass\n",
        "\n",
        "    # turn our list into an array\n",
        "    new_centroids = np.array(new_centroids)\n",
        "\n",
        "    return(new_centroids)\n"
      ],
      "metadata": {
        "id": "iAc9xU7vXYJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KM(df,k):\n",
        "\n",
        "    # 1. randomly select k centroids\n",
        "    centroids = choose_centroids(df,k)\n",
        "\n",
        "    converged = False # has the algorithm converged yet?\n",
        "    while not converged: # until the centroids stop moving\n",
        "\n",
        "        # 2. assign data points to a cluster\n",
        "        clust = choose_closest_cluster(centroids,df, k)\n",
        "\n",
        "        # 3. re-calculate the center/centroid of each cluster\n",
        "        new_centroids = recalculate_cluster_mean(clust,df, k)\n",
        "\n",
        "        # 4. check whether you can stop iterating by checking whether the\n",
        "        # distance between the previous position and current position is\n",
        "        # less than 0.0001 for all k centroids.\n",
        "\n",
        "        # calculate the distance between the old centroid values, and new_centroids values\n",
        "        change = np.array([np.linalg.norm(centroids[i]-new_centroids[i]) for i in range(0,k)])\n",
        "\n",
        "        # check whether all of them moved less than 0.0001 units.\n",
        "        converged = np.all(change < 0.0001)\n",
        "\n",
        "        # set new_centroids to be established centroids\n",
        "        centroids = new_centroids\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 5. Return cluster memberships dictionary, the structure\n",
        "    # should look like this (but can have different k, and different\n",
        "    # assignments depending on data/starting centroids/chosen )\n",
        "    # {0: [55, 72, 76, 85, 89, 93, 100, 104, 105, 107, 110, 119,\n",
        "#          123, 132, 144, 201, 202, 203, 204, 205, 206, 207, 209,\n",
        "#          210, 212, 213, 214, 215, 217, 218, 220, 221, 222, 223,\n",
        "#          225, 226, 227, 228, 229, 231, 232, 233, 234, 237, 238,\n",
        "#          241, 243, 245, 246, 247, 248, 249],\n",
        "#     1: [8, 47, 102, 103, 111, 114, 117, 120, 126, 129, 131, 136,\n",
        "#          141, 142, 143, 145, 146, 148],\n",
        "#     2: [51, 101, 106, 108, 109, 112, 113, 115, 116, 118, 121,\n",
        "#           122, 124, 125, 127, 128, 130, 133, 134, 135, 137, 138,\n",
        "#           139, 140, 147, 149, 200, 219, 224, 230, 235, 239, 240,\n",
        "#           242, 244],\n",
        "#     3: [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160,\n",
        "#           161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171,\n",
        "#           172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
        "#           183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193,\n",
        "#           194, 195, 196, 197, 198, 199, 216, 236],\n",
        "#     4: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16,\n",
        "#           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
        "#           31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,\n",
        "#           45, 46, 48, 49, 50, 52, 53, 54, 56, 57, 58, 59, 60, 61,\n",
        "#           62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77,\n",
        "#           78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 90, 91, 92, 94,\n",
        "#           95, 96, 97, 98, 99, 208, 211]}\n",
        "    return(clust)"
      ],
      "metadata": {
        "id": "wcO44DLQXg1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/programmers3.csv\")\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "U50hR04qXnq8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using your K-Means Function\n",
        "\n",
        "Now that you have done the incredibly impressive work of writing your own K-means function. Let's use it and compare the results to what we'd get from `sklearn`!\n",
        "\n",
        "First, use your OWN function `KM()` to do K-means on `data` with k = 5. Then generate the cluster assingments using the code provided. Then make a ggplot scatterplot of your clusters.\n",
        "\n",
        "Second, use sklearn's `KMeans()` function to do K-means on `data` with k = 5. Then generate the cluster assignments using `.predict()`. Then make a ggplot scatterplot of your clusters."
      ],
      "metadata": {
        "id": "bJXoZRwNXrAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run k-means\n",
        "\n",
        "# create features list\n",
        "feats = [\"py\", \"r\"]\n",
        "\n",
        "# z score\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# store z scored data in another data frame\n",
        "data_z = X.copy()\n",
        "data_z[feats] = pd.DataFrame(scaler.fit_transform(data[feats]))\n",
        "\n",
        "# use your function\n",
        "clusters = KM(data_z[feats], 5)\n",
        "\n",
        "# generate assignments\n",
        "assignments = np.array([999 for row in range(0, data_z.shape[0])])\n",
        "\n",
        "for cluster in clusters:\n",
        "    assignments[clusters[cluster]] = cluster\n",
        "\n",
        "data[\"assignments_ME\"] = assignments\n",
        "\n",
        "\n",
        "# create ggplot scatter plot of data, using x, y and color = \"assignments_ME\"\n",
        "(ggplot(data, aes(\"py\", \"r\", color = \"factor(assignments_ME)\")) +\n",
        "  geom_point() + theme_minimal() +\n",
        "  scale_color_discrete(name = \"Clusers\"))"
      ],
      "metadata": {
        "id": "1Ia-nv1fXv7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING SKLEARN\n",
        "\n",
        "# create kmeans model\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X[feats])\n",
        "\n",
        "\n",
        "m = KMeans(5)\n",
        "assignments = m.fit_predict(X_scaled)\n",
        "# add assignments to data\n",
        "data[\"assignments_SK\"] = assignments\n",
        "\n",
        "\n",
        "\n",
        "# create another ggplot scatter plot of data, using x, y and color = \"assignments_SK\"\n",
        "(ggplot(data, aes(\"py\", \"r\", color = \"factor(assignments_SK)\")) +\n",
        "geom_point() + theme_minimal() +\n",
        "scale_color_discrete(name = \"Cluster\"))\n"
      ],
      "metadata": {
        "id": "zcfIw14vXx9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Compare the results of each"
      ],
      "metadata": {
        "id": "7KzR1UfAX1q4"
      }
    }
  ]
}