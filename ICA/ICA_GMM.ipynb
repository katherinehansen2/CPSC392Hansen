{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1A9NNqOQJssk"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from plotnine import *\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GMM"
      ],
      "metadata": {
        "id": "QPCYNLrrK8ZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Review\n",
        "\n",
        "### Gaussian Mixture Models\n",
        "Expectation Maximization with Gaussian Mixture Models (called EM for short) is a clustering algorithm that's similar to k-means except it doesn't assume spherical variance within clusters. That means clusters can be ellipses rather than just spherical. For example the graph on the left shows roughly spherical clusters, whereas the graph on the right shows non-spherical clusters.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1BslkqKXSuxYNcpAhFLlsVsBSdWUCWY3W\"/>\n",
        "\n",
        "Discussion: Which algorithm would work better for the cluster on the right? Which algorithm would work better for the clusters on the left?\n",
        "\n",
        "\n",
        "The process for fitting EM is similar to k-means except for two main differences:\n",
        "\n",
        "1. Instead of estimating ONLY cluster means/centers, we also estimate the variance for each predictor.\n",
        "2. Instead of hard assignment (where each data point belongs to only 1 cluster), GMMs use soft assignment (where each data point has a probability of being in EACH cluster).\n",
        "    - because there is no hard assignment, the cluster centers/means and variances are calculated using EVERY data point weighted by the probability that the data point belongs to that cluster. Data points that are unlikely to belong to a cluster barely affect the center/mean and variance of that cluster, whereas data points that are very likely to belong to a cluster have a larger influence on the center/mean and variance of that cluster.\n",
        "\n",
        "This means that when clusters are NOT spherical, EM will be able to accomodate that, while k-means will not.\n",
        "\n",
        "### Math Takeaways\n",
        "We only do math when it'll help you understand the algorithm! So here are some takeaways this math should help you understand:\n",
        "\n",
        "- GMM does soft assignment, every data point belongs to every cluster with some probability\n",
        "- Data points that are more likely to be in a cluster have more influence over its parameters\n",
        "- GMM uses the EM algorithm to iteratively update the cluster distributions. It first assigning a responsibility to each data point (Expectation step), and then using them to calculate weighted means and variances for each cluster (Maximization step)\n",
        "- Responsibilities measure the probability of a data point being in each cluster (technically the posterior probability).\n",
        "- Responsibilities contain information about how common a cluster is as well as the likelihood of a data point belonging to that cluster\n"
      ],
      "metadata": {
        "id": "6qYsWaq-K-Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's redo our Beyonce clustering with GMM!"
      ],
      "metadata": {
        "id": "tCQIIO30MYq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the data + standardize\n",
        "beyonce = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/Beyonce_data.csv\")\n",
        "\n",
        "predictors = [\"energy\", \"danceability\", \"valence\"]\n",
        "\n",
        "X = beyonce[predictors]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 2. Create empty model\n",
        "gmm = #\n",
        "\n",
        "# 3. Fit model + predict\n",
        "labels = gmm.fit_predict(X_scaled)\n",
        "\n",
        "\n",
        "# Add cluster labels to the original DataFrame\n",
        "X[\"clusters\"] = labels\n",
        "\n",
        "# 5: Plot the results. We are plotting each combo of predictors\n",
        "print(ggplot(X, aes(x=\"energy\", y=\"danceability\", color=\"factor(clusters)\")) +\n",
        "      geom_point() + theme_minimal() +\n",
        "      labs(x=\"Energy\", y=\"Danceability\", title=\"GMM Clustering Results for K = 4\",\n",
        "           color=\"Clusters\"))\n",
        "\n",
        "print(ggplot(X, aes(x=\"energy\", y=\"valence\", color=\"factor(clusters)\")) +\n",
        "      geom_point() + theme_minimal() +\n",
        "      labs(x=\"Energy\", y=\"Valence\", title=\"GMM Clustering Results for K = 4\",\n",
        "           color=\"Clusters\"))\n",
        "\n",
        "print(ggplot(X, aes(x=\"valence\", y=\"danceability\", color=\"factor(clusters)\")) +\n",
        "      geom_point() + theme_minimal() +\n",
        "      labs(x=\"Valence\", y=\"Danceability\", title=\"GMM Clustering Results for K = 4\",\n",
        "           color=\"Clusters\"))"
      ],
      "metadata": {
        "id": "FLvCvn7WMYTD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like in K-Means, it is not always clear how many clusters to use. In K-Means, we calculated the silhouette scores of many Ks and selected the best. While you *can* calculate a silhouette score for a GMM model, it might not be the best option. GMM allows for oblong clusters, meaning cohesion + separation might not be the best choice for a performance metric.\n",
        "\n",
        "Another measure of cluster performance is the [**Bayesian Information Criterion**](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/mixture/_gaussian_mixture.py#L862) (BIC). The BIC measures how well fit your model is, where **lower** values of BIC are better.\n",
        "\n",
        "$$ BIC = \\underbrace{- 2 log(\\hat{L})}_\\text{goodness of fit} + \\underbrace{k*log(N)}_\\text{complexity penalty} $$\n",
        "\n",
        "- $\\hat{L}$ is the maximum likelihood of the model ($\\prod_{n = 1}^N \\sum_{k=1}^K w_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)$; from above)\n",
        "- $N$ is the number of data points\n",
        "- $k$ is the *number of parameters* in the model (just remember, the more clusters, the more parameters)\n",
        "\n",
        "When a clustering solution is *good*, it's likelihood will be *high*. So $-2 log(\\hat{L})$ will be *low*.\n",
        "\n",
        "\n",
        " BIC also **penalizes complexity** by adding on the $k*log(N)$ term. The more parameters we have to estimate ($k$) the higher $k*log(N)$ will be, thus BIC *penalizes* models for having a lot of parameters. If adding parameters doesn't improve the fit of the model (measured by $- 2 log(\\hat{L})$), we don't want them. This is similar to **LASSO** and **Ridge** penalties, where we have to have things \"pull their weight\" in order for them to be \"worth\" the penalty.\n",
        "\n",
        "In summary, we choose models with lower BIC values.\n",
        "\n"
      ],
      "metadata": {
        "id": "0o6thy0SM22f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try clustering the wine data and measuring the BIC scores."
      ],
      "metadata": {
        "id": "ecPU2PVuN1X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/wineLARGE.csv\")\n",
        "\n",
        "# drop and reset rows\n",
        "wine.dropna(inplace = True)\n",
        "wine.reset_index(inplace = True)\n",
        "\n",
        "# grab data we want to cluster\n",
        "feats = [\"citric.acid\", \"residual.sugar\"]\n",
        "\n",
        "X = wine[feats]\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# create dictionary to\n",
        "metrics = {\"bic\": [], \"k\": []}\n",
        "\n",
        "for i in range(2,20):\n",
        "    gmm = #\n",
        "    labels = gmm.fit_predict(X_scaled)\n",
        "    bic_score = #\n",
        "\n",
        "    metrics[\"bic\"].append(bic_score)\n",
        "    metrics[\"k\"].append(i)\n",
        "\n",
        "df = pd.DataFrame(metrics)\n"
      ],
      "metadata": {
        "id": "QGhyP8mYODQ9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ggplot(df, aes(x = \"k\", y = \"bic\")) +\n",
        "  geom_line() + theme_minimal() +\n",
        "    labs(x = \"K\", y = \"BIC Score\",\n",
        "         title = \"BIC Scores for Different Ks\"))"
      ],
      "metadata": {
        "id": "KfdRfVoIOrQH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gmm = GaussianMixture(#)\n",
        "\n",
        "labels = gmm.fit_predict(X_scaled)\n",
        "\n",
        "# Add cluster labels to the original DataFrame\n",
        "X[\"clusters\"] = labels\n",
        "\n",
        "# Plot the results. We are plotting each combo of predictors\n",
        "print(ggplot(X, aes(x = \"citric.acid\", y = \"residual.sugar\", color = \"factor(clusters)\")) +\n",
        "      geom_point() +\n",
        "      theme_minimal() +\n",
        "      scale_color_discrete(name = \"Cluster\") +\n",
        "      labs(x = \"Citric Acid\",\n",
        "           y = \"Residual Sugar\",\n",
        "           title = \"#TODO Cluster Solution\"))"
      ],
      "metadata": {
        "id": "7DvBPIveO0Nh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ICA\n",
        "\n",
        "\n",
        "Now, you're going to fit multiple clustering algorithms on each dataset below.\n",
        "\n",
        "For each dataset:\n",
        "- Make a ggplot of the data\n",
        "- fit a K-Means Model\n",
        "- fit a GMM\n",
        "- make a ggplot with colored clusters for each model\n",
        "\n",
        "Either choose k by making a plot and using your own judgement, or by trying out different k's and seeing which works best using the BIC.\n",
        "\n",
        "See how well GMM and KM perform. Do both do well? Does one do better than the other? Do both do poorly?\n",
        "\n",
        "### Very Distinct Clusters"
      ],
      "metadata": {
        "id": "6lWcUd5WPJkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/GMM_Classwork_01.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sXtXDTE1Zbml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means\n"
      ],
      "metadata": {
        "id": "aW-VI7aKPNaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GMM\n"
      ],
      "metadata": {
        "id": "ygDc8U_cPUGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Reflection\n",
        "\n",
        " Do both do well? Does one do better than the other? Do both do poorly?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UkeUXztrP2ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cluster in Cluster"
      ],
      "metadata": {
        "id": "7V1E3pGzPU0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d2 = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/KMEM4.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Hl4mFer-Z-nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means\n"
      ],
      "metadata": {
        "id": "EmQ5LGM0PW35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GMM\n"
      ],
      "metadata": {
        "id": "fPXWA5caPYP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Reflection\n",
        "\n",
        " Do both do well? Does one do better than the other? Do both do poorly?"
      ],
      "metadata": {
        "id": "ojuomZBHP7rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Oblong Clusters"
      ],
      "metadata": {
        "id": "3pLykMo6Pan1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d3 = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/GMM_Classwork_02.csv\")\n"
      ],
      "metadata": {
        "id": "PGigWpIsaNY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means\n"
      ],
      "metadata": {
        "id": "xX3gF8FOPcRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GMM\n"
      ],
      "metadata": {
        "id": "mbzuouK4Pcc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Reflection\n",
        "\n",
        " Do both do well? Does one do better than the other? Do both do poorly?"
      ],
      "metadata": {
        "id": "TBiTjmKPP8YO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clusters with different variances"
      ],
      "metadata": {
        "id": "14P46NSFPgp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d4 = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/GMM_Classwork_03.csv\")\n"
      ],
      "metadata": {
        "id": "1Y39gOE9aZPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means\n"
      ],
      "metadata": {
        "id": "r1PLoXCjPgDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GMM\n"
      ],
      "metadata": {
        "id": "sgNyK_ztPjg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Reflection\n",
        "\n",
        " Do both do well? Does one do better than the other? Do both do poorly?"
      ],
      "metadata": {
        "id": "rJA336YhP9HG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uneven sized Clusters"
      ],
      "metadata": {
        "id": "pQV-QtmfPlKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d5 = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/GMM_Classwork_04.csv\")\n"
      ],
      "metadata": {
        "id": "govw4xhjahLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means\n"
      ],
      "metadata": {
        "id": "tAAUDZeVPmxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GMM\n"
      ],
      "metadata": {
        "id": "edvEf6pgPnuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Reflection\n",
        "\n",
        " Do both do well? Does one do better than the other? Do both do poorly?"
      ],
      "metadata": {
        "id": "QPMVV7vEP9me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Reflection\n",
        "\n",
        "What cautions will you now take when doing K-Means? In other words, what issues did this classwork present that might change how you apply clustering to real data?"
      ],
      "metadata": {
        "id": "46feMvwLQADp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cluster Stability\n",
        "You may have already noticed this, but K-Means and EM will often give different solutions each time it runs. Run the following cells multiple times and notice how different (or not) the results are."
      ],
      "metadata": {
        "id": "hzIh6uEOQDbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d6 = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/KMEM6.csv\")\n",
        "\n",
        "feat = [\"x\", \"y\"]\n",
        "X = d6[feat]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "km = KMeans(n_clusters=9)\n",
        "pred = km.fit_predict(X_scaled)\n",
        "\n",
        "ggplot(d6, aes(\"x\", \"y\", color = pred)) + geom_point() + theme_minimal() + theme(legend_position = \"none\")"
      ],
      "metadata": {
        "id": "NnApaiURQNde"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d7 = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/KMEM5.csv\")\n",
        "\n",
        "feat = [\"x\", \"y\"]\n",
        "X = d7[feat]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "gmm = GaussianMixture(n_components=9)\n",
        "pred = gmm.fit_predict(X_scaled)\n",
        "\n",
        "ggplot(d7, aes(\"x\", \"y\", color = pred)) + geom_point() + theme_minimal() + theme(legend_position = \"none\")"
      ],
      "metadata": {
        "id": "Fyuozb7HQP-5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Reflection\n",
        "\n",
        "What do you think could cause this instability?"
      ],
      "metadata": {
        "id": "jx1hYA4Sbiok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Thoughts\n",
        "\n",
        "I hope this classwork doesn't scare you away from clustering. Clustering is an incredibly useful tool! However, it's not a perfect tool, and like all the other models we've learned, you have to be careful and thoughtful in how you apply it. Now that you've completed this classwork, you should have a much better idea of the cautions you may need to take."
      ],
      "metadata": {
        "id": "FCSRSYg3QRYU"
      }
    }
  ]
}