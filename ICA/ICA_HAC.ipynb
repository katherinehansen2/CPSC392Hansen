{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vIq0reDR-L2N"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from plotnine import *\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# make sure you have these to make dendrograms!-------\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from matplotlib import pyplot as plt\n",
        "#-----------------------------------------------------\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review\n",
        "\n",
        "Hierarchical clustering (as the name suggests) assumes that the clusters in the data have a hierarchical relationship. For example, in a McDonald's food dataset we could have clusters like: Dessert, Drinks, Sandwiches, Other. Within Sandwiches we could have chicken sandwiches, Burgers, Vegan Sandwiches...within Burgers we could have smaller, lower calorie options, and bigger, more substantial burgers...etc.\n",
        "\n",
        "Blood cells is another great example of a hierarchical relationship: ![blood hierarchy](https://community.jmp.com/t5/image/serverpage/image-id/16820i93FA5BD273E0A842/image-dimensions/340x314?v=1.0)\n",
        "\n",
        "Hierarchical *Agglomeretive* Clustering (which we perform here), goes bottom up: every datapoint starts as it's own singleton cluster, and at each step, we merge the two closest clusters together until all data points are in one big cluster. In order to decide which clusters are closest, we need to choose two things:\n",
        "\n",
        "\n",
        "## Distance Metrics and Linkage Critera\n",
        "\n",
        "* **distance metric**: this is a measure that helps us define how close together two *data points* are. Euclidean distance is a common distance metric that you may be familiar with, but there is also cosine distance, manhattan distance, hamming distance, and even custom distance functions (like [levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two strings!)\n",
        "\n",
        "* **linkage criteria**: this is a measure of how close two *clusters* are. Because (most) clusters have more than one point, we need to define what it means for two clusters to be close.\n",
        "    * **Single Linkage**: the distance between two clusters (A and B) as the minimum distance between any point in A and any point in B\n",
        "\n",
        "    ![single linkage](https://community.jmp.com/t5/image/serverpage/image-id/16823iF32133201794C0A4/image-dimensions/251x242?v=1.0)\n",
        "    * **Average Linkage**: the distance between two clusters (A and B) as the average distance between points in A and points in B.\n",
        "\n",
        "    ![average linkage](https://community.jmp.com/t5/image/serverpage/image-id/16824iDD065DCADD44D5EC/image-dimensions/275x307?v=1.0)\n",
        "    * **Complete Linkage**: the distance between two clusters (A and B) as the maximum distance between any point in A and any point in B.\n",
        "    \n",
        "    ![complete linkage](https://community.jmp.com/t5/image/serverpage/image-id/16825i39A778742E501081/image-dimensions/277x245?v=1.0)\n",
        "    * **Centroid Method**: the distance between two clusters (A and B) as the distance between their respective mean vectors (centroids).\n",
        "    \n",
        "    ![centroid method](https://community.jmp.com/t5/image/serverpage/image-id/16826iFC5E179AEFFF1252/image-dimensions/260x268?v=1.0)\n",
        "    * **Ward's Method** (default): the distance between two clusters (A and B) as the Sum of Squared Errors when combining the two clusters together.\n",
        "    \n",
        "    ![ward's method](https://community.jmp.com/t5/image/serverpage/image-id/16827iA35DD99890489DB2/image-dimensions/253x164?v=1.0)\n",
        "\n",
        "    * and MORE! You could technically define this anyway you wanted.\n",
        "\n",
        "  \n",
        "## Dendrograms\n",
        "\n",
        "### Diffuse Overlapping Clusters\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1OCGvoe2FtZdIm0NnXbuwDo49E9CDrkIc\" width = 600px />\n",
        "\n",
        "### Highly Separable Clusters\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1xpYaV-Pa1agH7H-OLzRvCcSWU-k78Uzf\" width = 600px />\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h2k8ppBCiR1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's Try on Our Own\n",
        "\n",
        "Chat with your table about the following dendrograms. Which do you think have highly separable data? Which do you think might have overlapping clusters? Discuss each dendrogram in detail.\n",
        "\n",
        "#### 1.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ilZW8x11EjSAYub7jFM_kN4DxLMsgvOx\" width = 500px />\n",
        "\n",
        "#### 2.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1bz5MM_HZe30uLLesgCXZkEhfcgmVHG43\" width = 500px />\n",
        "\n",
        "#### 3.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1b3VHTE0WqmgtVa8ywtJX6sQkvjVadQ6D\" width = 500px />\n",
        "\n"
      ],
      "metadata": {
        "id": "RQ7UiKwGi2Qb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sklearn\n",
        "\n",
        "Now time to try to build our own using sklearn!"
      ],
      "metadata": {
        "id": "8Wz1c8Z3jEHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tests_wide = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/testperform.csv\")\n",
        "\n",
        "tests_wide.head()"
      ],
      "metadata": {
        "id": "7sPnwOuVjKK_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up X/feats\n",
        "features = [\"zero\", \"one\", \"two\", \"three\", \"four\"]\n",
        "X = tests_wide[features]\n",
        "\n",
        "# Z-score\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "hac = AgglomerativeClustering(linkage = \"ward\",\n",
        "                              metric = \"euclidean\",\n",
        "                              distance_threshold=0,\n",
        "                              n_clusters = None) # come back and change the number of clusters\n",
        "\n",
        "# fit and get labels\n",
        "labels = hac.fit_predict(X_scaled)"
      ],
      "metadata": {
        "id": "W0mgY3M6lVtB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn: https://github.com/scikit-learn/scikit-learn/blob/70cf4a676caa2d2dad2e3f6e4478d64bcb0506f7/examples/cluster/plot_hierarchical_clustering_dendrogram.py\n",
        "def plot_dendrogram(hac, **kwargs):\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(hac.children_.shape[0])\n",
        "    n_samples = len(hac.labels_)\n",
        "    for i, merge in enumerate(hac.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack(\n",
        "        [hac.children_, hac.distances_, counts]\n",
        "    ).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    sch.dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "\n",
        "plot_dendrogram(hac, color_threshold = 5)"
      ],
      "metadata": {
        "id": "Uxnhgpm04H75"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll refit with a specific number of clusters"
      ],
      "metadata": {
        "id": "6wwitP9H4UnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hac = AgglomerativeClustering(linkage = \"ward\",\n",
        "                              metric = \"euclidean\",\n",
        "                              n_clusters = 3) # come back and change the number of clusters\n",
        "\n",
        "# fit and get labels\n",
        "labels = hac.fit_predict(X_scaled)\n",
        "\n",
        "# sil scor\n",
        "print(silhouette_score(X_scaled, labels))\n",
        "\n",
        "# look at cluster performance\n",
        "tests_wide[\"cluster_3\"] = labels\n",
        "gg_list = []\n",
        "for test in features:\n",
        "    title = \"Test \" + test.capitalize() + \" Cluster Performance\"\n",
        "    gg_list.append(ggplot(tests_wide, aes(x = \"factor(cluster_3)\", y = test))\n",
        "          + geom_boxplot(aes(fill = \"factor(cluster_3)\")) +\n",
        "          theme_minimal() +\n",
        "          scale_fill_discrete(name = \"Cluster Assignment\") +\n",
        "          labs(x = \"Cluster\",\n",
        "               y = \"Score\",\n",
        "               title = title))"
      ],
      "metadata": {
        "id": "RuuWjjQv4UJf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gg_list[0]"
      ],
      "metadata": {
        "id": "wEXC5lh841sE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gg_list[1]"
      ],
      "metadata": {
        "id": "7LuUHu3h47gU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gg_list[2]"
      ],
      "metadata": {
        "id": "vq5SAyvH47ya"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gg_list[3]"
      ],
      "metadata": {
        "id": "IOL1R8gf48Cm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gg_list[4]"
      ],
      "metadata": {
        "id": "MFhaGTIS48Oa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ICA\n",
        "\n",
        "## It's Election Day!\n",
        "\n",
        "If you haven't gotten a chance to vote, and you need some time to do it, please feel free to take this as an opportunity! There is a voting center in AF119 :)\n",
        "\n"
      ],
      "metadata": {
        "id": "2zhdklvGjVjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practicing HAC"
      ],
      "metadata": {
        "id": "C9fRgX8SjopA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Makeup example\n",
        "\n",
        "Let's look at some makeup purchasing data and cluster customers into groups.\n",
        "\n",
        "Use HAC with cosine distance to cluster customers based on `eyeshadow`, `lipstick`, and `foundation` purchases."
      ],
      "metadata": {
        "id": "FPKwa0-X52Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load in data\n",
        "makeup = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/makeup.csv\")\n",
        "makeup.head()\n",
        "\n",
        "# set up features\n",
        "features = [\"eyeshaddow\", \"lipstick\", \"foundation\"]\n",
        "\n",
        "# no need to z-score because it is count data\n",
        "X = makeup[features]\n",
        "\n",
        "# create empty model\n",
        "hac2 = AgglomerativeClustering(# TODO)\n",
        "\n",
        "\n",
        "# fit model and get labels\n",
        "labels = hac2.fit_predict(X[features])\n",
        "\n",
        "# assess\n",
        "plot_dendrogram(hac2, color_threshold= 0.015)"
      ],
      "metadata": {
        "id": "LltFDPz6koPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# refit with selected number of clusters\n",
        "\n"
      ],
      "metadata": {
        "id": "lNPLDxvYlSiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# draw spread boxplots for each feature\n",
        "# this is baseline code, be sure to update for your specific problem\n",
        "\n",
        "for test in features:\n",
        "    title = \"Test \" + test.capitalize() + \" Cluster Performance\"\n",
        "    gg_list.append(ggplot(tests_wide, aes(x = \"factor(cluster_3)\", y = test))\n",
        "          + geom_boxplot(aes(fill = \"factor(cluster_3)\")) +\n",
        "          theme_minimal() +\n",
        "          scale_fill_discrete(name = \"Cluster Assignment\") +\n",
        "          labs(x = \"Cluster\",\n",
        "               y = \"Score\",\n",
        "               title = title))"
      ],
      "metadata": {
        "id": "4zOVH1816TY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gg_list[0] # do for all items in list"
      ],
      "metadata": {
        "id": "7m2NhPh56eCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building your own distance functions\n",
        "\n",
        "TODO:\n",
        "Let's build a few functions that return the distance between two clusters. Each of these functions take in two dataframes, `A` and `B` that contain the datapoints for the two respective clusters (*number of features can vary*).\n",
        "\n",
        "The functions take in two arguments:\n",
        "\n",
        "* `A`: an N1 x P dataframe containing the data points in cluster A. (N1 is the number of data points in cluster A; P is the number of features used)\n",
        "* `B`: an N2 x P dataframe containing the data points in cluster B. (N2 is the number of data points in cluster B; P is the number of features used)\n",
        "\n",
        "\n",
        "The function should calculate and return the distance between the clusters (assume you're using euclidean distance for all of these) according to their respective linkage criterion (single, average, and complete). Remember a) that you need to calculate the distance between *every* point in `A` and *every* point in `B` and b) [`np.linalg.norm()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)."
      ],
      "metadata": {
        "id": "QPCKWSwDjqAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Discuss\n",
        "\n",
        "1. To calculate the distance between two clusters `A` (N1 x P) and `B` (N2 x P), how many distances (between 2 data points) would you have to calculate?"
      ],
      "metadata": {
        "id": "cDgCUVxH6qR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "def single(A,B):\n",
        "    pass\n",
        "\n",
        "\n",
        "def average(A,B):\n",
        "    pass\n",
        "\n",
        "\n",
        "def complete(A,B):\n",
        "    pass\n",
        "\n",
        "\n",
        "### /YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "1qaxscX8ko7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if your functions are working\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/HAC1.csv\")\n",
        "\n",
        "dA = df.loc[df.cluster == \"A\"] # cluster A\n",
        "dB = df.loc[df.cluster == \"B\"] # cluster B\n",
        "dC = df.loc[df.cluster == \"C\"] # cluster C"
      ],
      "metadata": {
        "id": "xnnGC94Y6xp_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if complete() is correct, this will print true\n",
        "completePASS = abs(complete(dA[[\"x\",\"y\"]], dB[[\"x\",\"y\"]]) - 4.718047025872837) <= 0.01\n",
        "print(\"Complete:\", completePASS)\n",
        "\n",
        "# if average() is correct, this will print true\n",
        "averagePASS = abs(average(dA[[\"x\",\"y\"]], dB[[\"x\",\"y\"]]) - 2.734811240314461) <= 0.01\n",
        "print(\"Average:\", averagePASS)\n",
        "\n",
        "# if single() is correct, this will print true\n",
        "singlePASS = abs(single(dA[[\"x\",\"y\"]], dB[[\"x\",\"y\"]]) - 0.7361237342164363) <= 0.01\n",
        "print(\"Single:\", singlePASS)"
      ],
      "metadata": {
        "id": "j0Pa3tx56zLt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Using the dataset `df` below,\n",
        "\n",
        "1. plot the clusters using ggplot, color by cluster\n",
        "2. use your functions `single()`, `average()`, and `complete()` to calculate the distances between each pair of clusters.\n",
        "\n",
        "3. Look at which clusters are considered \"close\" and \"far\" in different methods. Are there differences between which are furthest/closest between methods? What are they?\n",
        "\n",
        "4. Describe why you think you see these differences.\n"
      ],
      "metadata": {
        "id": "EGi8z46y8Aki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot\n"
      ],
      "metadata": {
        "id": "3gZ8ssxg8Hit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate distances (this will likely take a few min to run)\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "#---single------\n",
        "s_AB = ###\n",
        "s_AC = ###\n",
        "s_BC = ###\n",
        "\n",
        "print(\"AB:\", s_AB)\n",
        "print(\"AC:\", s_AC)\n",
        "print(\"BC:\", s_BC)\n",
        "print(\"\\n\")\n",
        "#---average-----\n",
        "a_AB = ###\n",
        "a_AC = ###\n",
        "a_BC = ###\n",
        "\n",
        "print(\"AB:\", a_AB)\n",
        "print(\"AC:\", a_AC)\n",
        "print(\"BC:\", a_BC)\n",
        "print(\"\\n\")\n",
        "#---complete----\n",
        "c_AB = ###\n",
        "c_AC = ###\n",
        "c_BC = ###\n",
        "\n",
        "print(\"AB:\", c_AB)\n",
        "print(\"AC:\", c_AC)\n",
        "print(\"BC:\", c_BC)\n",
        "print(\"\\n\")\n",
        "\n",
        "### /YOUR CODE HERE ###\n"
      ],
      "metadata": {
        "id": "RuvmxKF88IY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring Linkage Criteria"
      ],
      "metadata": {
        "id": "N-c8yNDN8MiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the predictors listed in `predictors`, perform HAC on the [burger king dataset](https://github.com/katherinehansen2/CPSC392Hansen/blob/main/data/burger-king-items.txt) using sklearn and the following linkage critera:\n",
        "\n",
        "* single linkage\n",
        "* average linkage\n",
        "* complete linkage\n",
        "* ward's method"
      ],
      "metadata": {
        "id": "jYSHbpXv8PAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "\n",
        "1. Plot and compare the different clusters/dendrograms that you get. What do you notice is similar/different?\n",
        "\n",
        "2. Discuss: Think hard: what do the dendrograms tell you about the data?\n",
        "\n",
        "(NOTE: see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) if you need a refresher on how to set linkage criteria in sklearn, and [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) for how to set it for the dendrogram)"
      ],
      "metadata": {
        "id": "2Lpa6-L08hmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "burg = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/burger-king-items.txt\",\n",
        "                  sep = \"\\t\")\n",
        "\n",
        "predictors = [\"Calories\", \"Protein(g)\", \"Fat(g)\", \"Sodium(mg)\", \"Carbs(g)\", \"Sugar(g)\"]\n",
        "\n",
        "X = burg[predictors]"
      ],
      "metadata": {
        "id": "Ojd7Ctq08OXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Single Linkage\n"
      ],
      "metadata": {
        "id": "0F5PEnKK8siS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Average Linkage\n"
      ],
      "metadata": {
        "id": "pSGhmi_98w15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Complete Linkage\n"
      ],
      "metadata": {
        "id": "rt9dsaFq8w8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Ward's Linkage\n"
      ],
      "metadata": {
        "id": "8awEZy6t8xC9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}