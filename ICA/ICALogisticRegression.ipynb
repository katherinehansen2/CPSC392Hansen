{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh3k8M4heqJC"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from plotnine import *\n",
        "\n",
        "# modelling\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer #Z-score variables\n",
        "\n",
        "# performance\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix,\\\n",
        " f1_score, recall_score, precision_score, roc_auc_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# model validation imports\n",
        "from sklearn.model_selection import train_test_split # simple TT split cv\n",
        "from sklearn.model_selection import KFold # k-fold cv\n",
        "from sklearn.model_selection import LeaveOneOut #LOO cv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "We spoke about several metrics you can use to interpret how well your logistic regression model.\n",
        "\n",
        "- Accuracy: $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
        "- Confusion Matrix Patterns\n",
        "\n",
        "|                 | **Actually 1**          | **Actually 0**          |\n",
        "|-----------------|-------------------------|-------------------------|\n",
        "| **Predicted 1** | True Positive **(TP)**  | False Positive **(FP)** |\n",
        "| **Predicted 0** | False Negative **(FN)** | True Negative **(TN)**  |\n",
        "\n",
        "\n",
        "- **Precision**: $\\frac{TP}{TP + FP}$, how many of the predicted positives are true positives?\n",
        "\n",
        "- **Recall/Sensitivity**: $\\frac{TP}{TP + FN}$, how many of the actual positives did we accurately predict?\n",
        "\n",
        "- **Specificity**: $\\frac{TN}{TN + FP}$, how many of the actual negatives did we accurately predict?\n",
        "\n",
        "- **F1 Score**: $\\frac{2 * Precision * Recall}{Precision + Recall}$, a combination of precision and recall.\n",
        "\n",
        "- **ROC AUC**: The area under the ROC curve which puts the False Positive Rate (FPR) on the x-axis, and the True Positive Rate on the y-axis.\n",
        "\n",
        "\n",
        "The metrics we talked about were not an exhaustive list. [Here](https://scikit-learn.org/stable/api/sklearn.metrics.html#module-sklearn.metrics) are all of the metrics that sklearn has implemented.\n",
        "\n",
        "\n",
        "TODO:\n",
        "If you were designing a Flu test, which of the metrics we spoke about in class would be most important to you (there's no one right answer) and why?"
      ],
      "metadata": {
        "id": "AZYPsOZ7e-U6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC-AUC\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1n-Pg6y8wD-UM05kPyVKorbmteMpHdmcG\" alt=\"ROCAUC Curve\" width = \"600\"/>\n",
        "\n",
        "\n",
        "TODO:\n",
        "\n",
        "Is the ROC AUC of this model closer to 0.5 or 1? How did you decide that?"
      ],
      "metadata": {
        "id": "-8717shsf3qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calibration\n",
        "\n",
        "If you'd like to use a model's predicted probabilities, calibration is important.\n",
        "\n",
        "\n",
        "TODO:\n",
        "\n",
        "Discuss with your table group the following question: Is calibration important for an *accurate* prediction?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zq6yKwyLgO56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Building\n",
        "\n",
        "TODO:\n",
        "\n",
        "Using the Lizzo dataset on GitHub, build a logistic regression model to predict the mode. Then check the performance and calibration"
      ],
      "metadata": {
        "id": "Kqs5TES1gxgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/katherinehansen2/CPSC392Hansen/refs/heads/main/data/Lizzo_data.csv\")\n",
        "\n",
        "# turn boolean into 0's and 1's (not necessary, but helps with confusion over True/False)\n",
        "df[\"explicit\"] = df[\"explicit\"].astype(\"int\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "S_ovQtBNf0sB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictors = [\"danceability\", \"energy\", \"instrumentalness\", \"explicit\"]\n",
        "contin = [\"danceability\", \"energy\", \"instrumentalness\"]\n",
        "\n",
        "X = df[predictors]\n",
        "y = df[\"mode\"]\n",
        "\n",
        "# TTS\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# z-score continuous predictors only\n",
        "scaler = StandardScaler()\n",
        "X_train[contin] = scaler.fit_transform(X_train[contin])\n",
        "X_test[contin] = scaler.transform(X_test[contin])\n",
        "\n",
        "# Create logistic regression model\n",
        "lr = #TODO\n",
        "\n",
        "# fit logistic regression model\n",
        "#TODO\n"
      ],
      "metadata": {
        "id": "cMbcycqzhNCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions for train and test\n",
        "# with logistic regression, we can return both the predicted class AND the predicted probabilities\n",
        "y_pred_train = lr.predict(X_train)\n",
        "y_pred_test = lr.predict(X_test)\n",
        "\n",
        "y_pred_train_prob = lr.predict_proba(X_train)[:,1]\n",
        "y_pred_test_prob = lr.predict_proba(X_test)[:,1]"
      ],
      "metadata": {
        "id": "9_shK5qzhrOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assess\n",
        "print(\"Train Acc       : \", accuracy_score(y_train, y_pred_train))\n",
        "print(\"Train Prescision: \", precision_score(y_train, y_pred_train))\n",
        "print(\"Train Recall    : \", recall_score(y_train, y_pred_train))\n",
        "print(\"Train F1        : \", f1_score(y_train, y_pred_train))\n",
        "print(\"Train ROC AUC   : \", roc_auc_score(y_train, y_pred_train_prob))\n",
        "\n",
        "\n",
        "print(\"Test Acc        : \", accuracy_score(y_test, y_pred_test))\n",
        "print(\"Test Prescision : \", precision_score(y_test, y_pred_test))\n",
        "print(\"Test Recall     : \", recall_score(y_test, y_pred_test))\n",
        "print(\"Test F1         : \", f1_score(y_test, y_pred_test))\n",
        "print(\"Test ROC AUC    : \", roc_auc_score(y_test, y_pred_test_prob))"
      ],
      "metadata": {
        "id": "BLvs8avwh-gP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_train,y_pred_train)"
      ],
      "metadata": {
        "id": "7wobHWCwiB2g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC AUC\n",
        "RocCurveDisplay.from_predictions(y_train, y_pred_train_prob)"
      ],
      "metadata": {
        "id": "XzirGFACiF-O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calibration Curve\n",
        "prob_true, prob_pred = calibration_curve(y_train, y_pred_train_prob, n_bins=10)\n",
        "\n",
        "(ggplot() + geom_line(aes(x = prob_pred, y = prob_true), color = \"blue\")\n",
        "+ geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = \"dashed\") +\n",
        "labs(x = \"Average Predicted Probability\",\n",
        "     y = \"Average Acual Probability\",\n",
        "     title = \"Calibration Curve\") +\n",
        "ylim([0,1]) + xlim([0,1]))"
      ],
      "metadata": {
        "id": "CxUaAQmNiKbW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "\n",
        "Discuss in detail the performance of your model based on at least 3 of the metrics above. Then, discuss how well calibrated your model is."
      ],
      "metadata": {
        "id": "hl9iJN8DiTjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coefficients\n",
        "\n",
        "coefficients = pd.DataFrame({\n",
        "    \"Coef\": lr.coef_[0],  # grab array of coefficients\n",
        "    \"Name\": predictors\n",
        "})\n",
        "\n",
        "intercept = pd.DataFrame({\n",
        "    \"Coef\": lr.intercept_[0],  # grab intercept\n",
        "    \"Name\": \"intercept\"\n",
        "}, index=[coefficients.shape[0]])  # assign row index\n",
        "\n",
        "\n",
        "coefficients_all = pd.concat([coefficients, intercept])\n",
        "\n",
        "# create odds column\n",
        "coefficients_all[\"Odds\"] = np.exp(coefficients_all[\"Coef\"])\n",
        "\n",
        "coefficients_all"
      ],
      "metadata": {
        "id": "gLhqxs90iT-C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Interpret the coefficients above."
      ],
      "metadata": {
        "id": "5vXzqT3BjETt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another Logistic Regression example\n",
        "TODO:\n",
        "\n",
        "Now let's do the same thing but with [this dataset](https://www.kaggle.com/datasets/ruthgn/wine-quality-data-set-red-white-wine). Download the data from Kaggle, and upload it to Colab.\n",
        "\n",
        "Let's build a model that predicts whether a wine is `red` or `white` using *all* the other variables.\n",
        "\n",
        "(NOTE: because the column `type` is a string not a binary `0`/`1` variable, we'll need to use [`LabelBinarizer()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) to turn it into a binary variable.)"
      ],
      "metadata": {
        "id": "lg-beK5VjMBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load in data\n",
        "w = pd.read_csv(\"wine-quality-white-and-red.csv\")\n",
        "# w.head()\n",
        "\n",
        "# check missing\n",
        "\n",
        "\n",
        "# drop\n",
        "\n",
        "\n",
        "# Binarize and X and Y\n",
        "\n",
        "\n",
        "\n",
        "# TTS\n",
        "\n",
        "# Create Empty Model\n",
        "\n",
        "\n",
        "# fit\n",
        "\n",
        "\n",
        "# predict\n",
        "\n"
      ],
      "metadata": {
        "id": "7CrLShWRjT1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assess - pick 3+ metrics to assess"
      ],
      "metadata": {
        "id": "1JFfH09gjXzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calibration"
      ],
      "metadata": {
        "id": "UoTGWj6fjaUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Math of Log Odds, Odds, and Probabilities\n",
        "\n",
        "In the lecture, you saw a graph demonstrating the way that Log Odds, Odds, and Probabilities change as a result of a 1-unit change in a predictor variable.\n",
        "\n",
        "Now YOU'RE going to make your own version to help you gain an intuitive sense for the math of log odds, odds, and probabilities.\n",
        "\n",
        "We're going to write a function that simulates a super simple logistic relationship: 1 predictor (X) and 1 binary outcome (y).\n",
        "\n",
        "(if it helps, think of this as predicting whether or not someone is registered to vote based on their age.)"
      ],
      "metadata": {
        "id": "fweWvwzNjoJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LogisticDataSimulator(intercept, slope, limits = 5):\n",
        "\n",
        "    #generate 1000 evenly spaced values between -limits and limits\n",
        "    predictor = np.linspace(-limits, limits, 1000)\n",
        "\n",
        "    # log odds of being registered to vote\n",
        "    log_odds = intercept + slope*predictor\n",
        "\n",
        "\n",
        "    # odds of being registered to vote\n",
        "    odds = np.exp(log_odds)\n",
        "\n",
        "\n",
        "    # probability of being registered to vote\n",
        "    probabilities =  odds/(1 + odds)\n",
        "\n",
        "\n",
        "\n",
        "    # put into a dataframe\n",
        "    df = pd.DataFrame({\"x\": predictor,\n",
        "                      \"logodds\": log_odds,\n",
        "                       \"odds\": odds,\n",
        "                       \"probabilities\": probabilities})\n",
        "\n",
        "    return(df)"
      ],
      "metadata": {
        "id": "hJV2h3khjwSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the dataframe that calling `LogisticDataSimulator()` with an intercept of 1 and slope of 0.5 makes, and recreate the graph of the log odds, odds, and probabilities from the lecture (see slide 69). Make separate graphs for logodds, odds, and probabilities to make your life easier.\n",
        "\n",
        "Be sure to include the colored lines, the titles/axis labels, and the dotted vertical lines! Don't be afraid to google how to do things in ggplot (hint: google `geom_line()` and `geom_vline()`)."
      ],
      "metadata": {
        "id": "0XQsNg-TjyzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data = LogisticDataSimulator(intercept = 1, slope = 0.5)\n",
        "\n",
        "#################################\n",
        "# TODO\n",
        "#################################"
      ],
      "metadata": {
        "id": "VuAUNZ9KjzQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Describe in detail how the graphs demonstrate what we learned in class: that the change in predicted probability in response to a 1 unit increase of our predictor is *not constant*? Why might this cause an issue?\n",
        "\n"
      ],
      "metadata": {
        "id": "4scmFMb-j46Z"
      }
    }
  ]
}